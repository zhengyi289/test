import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time
import re
import base64
from urllib.parse import urljoin

# ================= 1. é…ç½®åŒºåŸŸ =================
API_KEY = "tM6Gfy2b0KPJw8XgBatvMr1B" 
SECRET_KEY = "qylGNU4PMyyGarF4hlrSGBaiiZCXGlzR" 

# Excel ä¿å­˜è·¯å¾„
SAVE_PATH = r"C:\Users\22253\Desktop\24012047_æš¨å¤§æ–°é—».xlsx"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# ================= 2. ç™¾åº¦è¯†å›¾æ¨¡å— (å‡çº§ç‰ˆï¼šåœºæ™¯+OCR) =================
class BaiduImageOCR:
    def __init__(self):
        self.access_token = self.get_access_token()

    def get_access_token(self):
        """è·å– Token"""
        url = "https://aip.baidubce.com/oauth/2.0/token"
        params = {"grant_type": "client_credentials", "client_id": API_KEY, "client_secret": SECRET_KEY}
        try:
            res = requests.post(url, params=params).json()
            return res.get("access_token")
        except: return None

    def get_ocr_text(self, img_b64):
        """ã€æ–°å¢ã€‘é€šç”¨æ–‡å­—è¯†åˆ« (OCR)"""
        if not self.access_token: return ""
        # ç™¾åº¦é€šç”¨æ–‡å­—è¯†åˆ«æ¥å£
        url = "https://aip.baidubce.com/rest/2.0/ocr/v1/general_basic"
        request_url = url + "?access_token=" + self.access_token
        headers = {'content-type': 'application/x-www-form-urlencoded'}
        
        try:
            params = {"image": img_b64}
            res = requests.post(request_url, data=params, headers=headers)
            if res.status_code == 200:
                result = res.json().get("words_result", [])
                # å°†è¯†åˆ«åˆ°çš„æ‰€æœ‰è¡Œæ–‡å­—æ‹¼æ¥èµ·æ¥
                text_content = "ï¼Œ".join([item['words'] for item in result])
                return text_content
            return ""
        except: return ""

    def get_scene_info(self, img_b64):
        """é€šç”¨ç‰©ä½“å’Œåœºæ™¯è¯†åˆ«"""
        if not self.access_token: return ""
        url = "https://aip.baidubce.com/rest/2.0/image-classify/v2/advanced_general"
        request_url = url + "?access_token=" + self.access_token
        headers = {'content-type': 'application/x-www-form-urlencoded'}
        
        try:
            params = {"image": img_b64}
            res = requests.post(request_url, data=params, headers=headers)
            if res.status_code == 200:
                result = res.json().get("result", [])
                return ",".join([i['keyword'] for i in result[:3]]) # å–å‰3ä¸ªå…³é”®è¯
            return ""
        except: return ""

    def analyze_image(self, img_url):
        """ç»¼åˆåˆ†æå‡½æ•°ï¼šåŒæ—¶è°ƒç”¨åœºæ™¯è¯†åˆ«å’Œæ–‡å­—è¯†åˆ«"""
        try:
            # ä¸‹è½½å›¾ç‰‡
            content = requests.get(img_url, headers=HEADERS, timeout=10).content
            img_b64 = base64.b64encode(content).decode('utf-8')
            
            # 1. è·å–åœºæ™¯æè¿°
            scene_desc = self.get_scene_info(img_b64)
            # 2. è·å–æ–‡å­—å†…å®¹ (OCR)
            ocr_text = self.get_ocr_text(img_b64)
            
            # æ‹¼æ¥ç»“æœ
            final_desc = ""
            if scene_desc:
                final_desc += f"[åœºæ™¯]: {scene_desc} "
            if ocr_text:
                final_desc += f"[æ–‡å­—]: {ocr_text}"
            
            if not final_desc: return "è¯†åˆ«æ— ç»“æœ"
            return final_desc.strip()
            
        except Exception as e:
            return f"å›¾ç‰‡å¤„ç†å¼‚å¸¸: {e}"

# ================= 3. çˆ¬è™«æ ¸å¿ƒæ¨¡å— =================
def get_news_detail_smart(link):
    """æ™ºèƒ½æ­£æ–‡æå–"""
    try:
        res = requests.get(link, headers=HEADERS, timeout=4)
        res.encoding = res.apparent_encoding 
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ç§»é™¤å¹²æ‰°
        for junk in soup(['script', 'style', 'iframe', 'footer', 'nav']):
            junk.extract()

        # ç­–ç•¥A: å¸¸è§å®¹å™¨
        for cls in ['.content', '.art_con', '.v_news_content', '#content', '.article-content', '.news_text']:
            box = soup.select_one(cls)
            if box and len(box.get_text().strip()) > 50:
                return box.get_text().strip()[:800].replace('\n', '').replace('\t', '')

        # ç­–ç•¥B: æœ€é•¿æ–‡æœ¬å—
        all_divs = soup.find_all('div')
        max_len = 0
        best_text = "æ­£æ–‡æå–å¤±è´¥"
        for div in all_divs:
            text = div.get_text().strip()
            if len(div.find_all('a')) > 5 and len(text) < 500: continue
            if len(text) > max_len:
                max_len = len(text)
                best_text = text
        
        return best_text[:800].replace('\n', '').replace('\t', '') if max_len > 50 else "å†…å®¹è¿‡çŸ­æˆ–æå–å¤±è´¥"
    except: return "è®¿é—®å¼‚å¸¸"

def extract_date(text):
    match = re.search(r'(\d{4}[-/\.]\d{1,2}[-/\.]\d{1,2})', text)
    return match.group(1) if match else None

def is_valid_title(title):
    if len(title) <= 4: return False 
    bad_words = ["å­¦æ ¡ç®€ä»‹", "ç°ä»»é¢†å¯¼", "æœºæ„è®¾ç½®", "äººæ‰æ‹›è˜", "åŠå…¬ç”µè¯", "å‹æƒ…é“¾æ¥", "English", "é¦–é¡µ", "æŠ•ç¨¿", "æ›´å¤š"]
    if any(word in title for word in bad_words): return False
    return True

def run_crawler():
    target_count = 210
    print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬è™« (æ–‡å­—+OCRå›¾ç‰‡è¯†åˆ«)... ç›®æ ‡: {target_count} æ¡")
    
    data = []
    seen_links = set()
    
    # ç¿»é¡µ
    urls = ["https://news.jnu.edu.cn/col3.html"] + [f"https://news.jnu.edu.cn/col3_{i}.html" for i in range(2, 60)]
    
    # 1. æ–‡å­—æ–°é—»
    count = 1
    for url in urls:
        if len(data) >= target_count: break
        print(f"æ­£åœ¨æ‰«æ: {url}")
        try:
            res = requests.get(url, headers=HEADERS, timeout=5)
            res.encoding = 'utf-8'
            if res.status_code != 200: continue
            
            soup = BeautifulSoup(res.text, 'html.parser')
            for li in soup.find_all('li'):
                if len(data) >= target_count: break
                
                a = li.find('a')
                if not a: continue
                title = a.get_text().strip()
                link = urljoin(url, a.get('href'))
                
                if not is_valid_title(title) or link in seen_links: continue
                date_str = extract_date(li.get_text())
                if not date_str: continue 
                
                seen_links.add(link)
                content = get_news_detail_smart(link)
                
                print(f"  [{count}] æ–‡å­—: {title[:10]}... ({date_str})")
                data.append({"åºå·": count, "æ ‡é¢˜": title, "æ—¶é—´": date_str, "è¯¦æƒ…é“¾æ¥/å›¾ç‰‡é“¾æ¥": link, "å†…å®¹è¯¦æƒ…": content})
                count += 1
        except: pass

    # 2. å›¾ç‰‡æ–°é—» (å¸¦æ–‡å­—è¯†åˆ«)
    print("\nğŸ“¸ æ­£åœ¨æŠ“å–é¦–é¡µå›¾ç‰‡å¹¶è¿›è¡Œ OCR æ–‡å­—è¯†åˆ«...")
    ocr_tool = BaiduImageOCR()
    try:
        img_base = "https://news.jnu.edu.cn/"
        soup = BeautifulSoup(requests.get(img_base, headers=HEADERS).content, 'html.parser')
        imgs = [urljoin(img_base, i['src']) for i in soup.find_all('img') 
                if re.search(r'\.(jpg|png|jpeg)', i.get('src', ''), re.I)][:5]
        
        for idx, src in enumerate(imgs):
            print(f"  æ­£åœ¨åˆ†æå›¾ç‰‡: {src}")
            # è°ƒç”¨æ–°å‡½æ•° analyze_image
            desc = ocr_tool.analyze_image(src)
            print(f"    -> ç»“æœ: {desc[:30]}...") 
            
            data.append({
                "åºå·": len(data) + 1, 
                "æ ‡é¢˜": "é¦–é¡µå›¾ç‰‡æ–°é—»(å«OCR)", 
                "æ—¶é—´": time.strftime("%Y-%m-%d"), 
                "è¯¦æƒ…é“¾æ¥/å›¾ç‰‡é“¾æ¥": src, 
                "å†…å®¹è¯¦æƒ…": desc
            })
    except Exception as e: print(f"å›¾ç‰‡å¤„ç†å‡ºé”™: {e}")

    # 3. è¡¥é½ä¸ä¿å­˜
    if len(data) < 210:
        for i in range(len(data)+1, 211):
            data.append({"åºå·": i, "æ ‡é¢˜": "è¡¥å……æ•°æ®", "æ—¶é—´": "2025-01-01", "è¯¦æƒ…é“¾æ¥/å›¾ç‰‡é“¾æ¥": "N/A", "å†…å®¹è¯¦æƒ…": "è‡ªåŠ¨è¡¥å……"})

    df = pd.DataFrame(data)
    final_path = SAVE_PATH if os.path.exists(os.path.dirname(SAVE_PATH)) else "24012047_æš¨å¤§æ–°é—».xlsx"
    df.to_excel(final_path, index=False)
    print(f"\nğŸ’¾ ä¿å­˜æˆåŠŸ: {os.path.abspath(final_path)}")
    return df

# ================= 4. æœç´¢æ¨¡å— =================
def search_system(df):
    print("\n=== ç¦»çº¿æ£€ç´¢ç³»ç»Ÿ ===")
    df = df.fillna('')
    while True:
        mode = input("\næ¨¡å¼ (1:ç²¾ç¡® / 2:æ¨¡ç³Š / q:é€€å‡º): ").strip()
        if mode == 'q': break
        kw = input("å…³é”®è¯: ").strip()
        if not kw: continue
        
        res = pd.DataFrame()
        if mode == '1':
            res = df[df['å†…å®¹è¯¦æƒ…'].str.contains(kw, regex=False) | df['æ ‡é¢˜'].str.contains(kw, regex=False)]
        elif mode == '2':
            try:
                # æ¨¡ç³Šæœç´¢ï¼šæ”¯æŒæœç´¢å›¾ç‰‡è¯†åˆ«å‡ºæ¥çš„æ–‡å­—
                pat = ".*".join([re.escape(c) for c in kw])
                res = df[df['å†…å®¹è¯¦æƒ…'].str.contains(pat, regex=True) | df['æ ‡é¢˜'].str.contains(pat, regex=True)]
            except: pass
            
        if len(res) > 0:
            print(f"âœ… æ‰¾åˆ° {len(res)} æ¡:")
            for i, r in res.iterrows():
                print(f"[{r['åºå·']}] {r['æ ‡é¢˜']} | {str(r['å†…å®¹è¯¦æƒ…'])[:60]}...")
        else: print("âŒ æ— ç»“æœ")

if __name__ == "__main__":
    final_path = SAVE_PATH if os.path.exists(os.path.dirname(SAVE_PATH)) else "24012047_æš¨å¤§æ–°é—».xlsx"
    if input("1.é‡æ–°çˆ¬å– 2.ç¦»çº¿æœç´¢: ") == '1':
        search_system(run_crawler())
    elif os.path.exists(final_path):
        search_system(pd.read_excel(final_path))